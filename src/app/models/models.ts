// ./src/app/models.ts

export enum UserRole {
    ADMIN = 'ADMIN',
    USER = 'USER',
    GUEST = 'GUEST'
}

export class User {
    constructor(
        public id: number,
        public name: string,
        public email: string,
        // public role: UserRole,
        // public profilePictureUrl: string
    ) { }
}
export class TwoFactorAuthDetails {
    constructor(
        public userId: number,
        public secret: string,
        public qrCodeUrl: string
    ) { }
}

export interface Thread {
    title: string;
    timestamp: number;
    description: string;
    body: string;
    isEdit: boolean;
}

export type GPTModels = 'gpt-4-vision-preview' | 'gemini-1.5-flash' | 'gemini-1.5-pro' | 'gemini-1.0-pro' | 'gemini-1.0-pro-vision';

export type ChatCompletionContentPart = ChatCompletionContentPartText | ChatCompletionContentPartImage;
export interface ChatCompletionContentPartImage { image_url: ChatCompletionContentPartImage.ImageURL; type: 'image_url'; }
export namespace ChatCompletionContentPartImage { export interface ImageURL { url: string; detail?: 'auto' | 'low' | 'high'; label?: string, second?: number } }
export interface ChatCompletionContentPartText { text: string; type: 'text'; }

// export type Message = ({ role: 'system', content: string } | { role: 'user' | 'assistant', content: string | ChatCompletionContentPart[] });
export type Message = { role: 'system' | 'user' | 'assistant', content: ChatCompletionContentPart[] };
export type MessageForView = Message & { editing?: number, status?: number, cached?: number };

export interface CachedContent {
    name: string;
    model: string;
    createTime: string;
    updateTime: string;
    expireTime: string;
}

export interface ChatCompletionStreamInDto {
    args: {
        messages: Message[],
        model?: GPTModels,
        temperature?: number,
        top_p?: number,
        response_format?: { type: 'text' | 'json_object' },
        stream?: true,

        /**
         * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
         * existing frequency in the text so far, decreasing the model's likelihood to
         * repeat the same line verbatim.
         *
         * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)
         */
        frequency_penalty?: number | null;

        /**
         * Modify the likelihood of specified tokens appearing in the completion.
         *
         * Accepts a JSON object that maps tokens (specified by their token ID in the
         * tokenizer) to an associated bias value from -100 to 100. Mathematically, the
         * bias is added to the logits generated by the model prior to sampling. The exact
         * effect will vary per model, but values between -1 and 1 should decrease or
         * increase likelihood of selection; values like -100 or 100 should result in a ban
         * or exclusive selection of the relevant token.
         */
        logit_bias?: Record<string, number> | null;

        /**
         * The maximum number of [tokens](/tokenizer) to generate in the chat completion.
         *
         * The total length of input tokens and generated tokens is limited by the model's
         * context length.
         * [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
         * for counting tokens.
         */
        max_tokens?: number | null;

        /**
         * How many chat completion choices to generate for each input message.
         */
        n?: number | null;

        /**
         * Number between -2.0 and 2.0. Positive values penalize new tokens based on
         * whether they appear in the text so far, increasing the model's likelihood to
         * talk about new topics.
         *
         * [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/gpt/parameter-details)
         */
        presence_penalty?: number | null;

        // /**
        //  * An object specifying the format that the model must output.
        //  *
        //  * Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the
        //  * message the model generates is valid JSON.
        //  *
        //  * **Important:** when using JSON mode, you **must** also instruct the model to
        //  * produce JSON yourself via a system or user message. Without this, the model may
        //  * generate an unending stream of whitespace until the generation reaches the token
        //  * limit, resulting in increased latency and appearance of a "stuck" request. Also
        //  * note that the message content may be partially cut off if
        //  * `finish_reason="length"`, which indicates the generation exceeded `max_tokens`
        //  * or the conversation exceeded the max context length.
        //  */
        // response_format?: ChatCompletionCreateParams.ResponseFormat;

        /**
         * This feature is in Beta. If specified, our system will make a best effort to
         * sample deterministically, such that repeated requests with the same `seed` and
         * parameters should return the same result. Determinism is not guaranteed, and you
         * should refer to the `system_fingerprint` response parameter to monitor changes
         * in the backend.
         */
        seed?: number | null;

        /**
         * Up to 4 sequences where the API will stop generating further tokens.
         */
        stop?: string | null | Array<string>;

        // Gemini用:コンテキストキャッシュ
        cachedContent?: CachedContent,
    };
    options?: {
        idempotencyKey: string
    };
}
